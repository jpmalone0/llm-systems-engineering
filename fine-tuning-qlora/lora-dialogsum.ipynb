{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jpmalone/Documents/Repos/llm-systems-engineering/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load-dataset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples: 12460\n",
      "Test examples: 1500\n",
      "\n",
      "Example:\n",
      "Dialogue: #Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\n",
      "#Person2#: I found it would be a good idea to get a check-up.\n",
      "#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\n",
      "#Person2#: I know. I figure as long as there is nothing wrong, why go see the doc...\n",
      "\n",
      "Summary: Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"knkarthick/dialogsum\")\n",
    "\n",
    "print(f\"Train examples: {len(dataset['train'])}\")\n",
    "print(f\"Test examples: {len(dataset['test'])}\")\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"Dialogue: {dataset['train'][0]['dialogue'][:300]}...\")\n",
    "print(f\"\\nSummary: {dataset['train'][0]['summary']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "load-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "Model parameters: 1,100,048,384\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load model based on device\n",
    "if device == \"cuda\":\n",
    "    # Use 4-bit quantization on CUDA for memory efficiency\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "elif device == \"mps\":\n",
    "    # MPS (Apple Silicon) - load in float32, move to MPS\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        dtype=torch.float32,\n",
    "        low_cpu_mem_usage=True,\n",
    "    ).to(device)\n",
    "else:\n",
    "    # CPU: load in float32\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        dtype=torch.float32,\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "\n",
    "print(f\"Model loaded: {MODEL_NAME}\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "lora-config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.1023\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,                  \n",
    "    lora_alpha=16,        \n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "format-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 50 examples\n",
      "\n",
      "Formatted example:\n",
      "<|system|>\n",
      "You are a helpful assistant that summarizes conversations.</s>\n",
      "<|user|>\n",
      "Summarize the following conversation:\n",
      "\n",
      "#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\n",
      "#Person2#: I found it would be a good idea to get a check-up.\n",
      "#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\n",
      "#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\n",
      "#Person1#: Well, the best way to avoid serious illnesses is to find out a...\n"
     ]
    }
   ],
   "source": [
    "def format_example(example):\n",
    "    \"\"\"Format dialogue and summary into instruction format.\"\"\"\n",
    "    text = f\"\"\"<|system|>\n",
    "You are a helpful assistant that summarizes conversations.</s>\n",
    "<|user|>\n",
    "Summarize the following conversation:\n",
    "{example['dialogue']}</s>\n",
    "<|assistant|>\n",
    "{example['summary']}</s>\"\"\"\n",
    "    \n",
    "    return {\"text\": text}\n",
    "\n",
    "# Format datasets\n",
    "train_dataset = dataset[\"train\"].map(format_example)\n",
    "test_dataset = dataset[\"test\"].map(format_example)\n",
    "\n",
    "train_subset = train_dataset.select(range(300))\n",
    "\n",
    "print(f\"Training on {len(train_subset)} examples\")\n",
    "print(f\"\\nFormatted example:\\n{train_subset[0]['text'][:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-before",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DIALOGUE:\n",
      "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
      "#Person2#: Yes, sir...\n",
      "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
      "#Person2#: Yes, sir. Go ahead.\n",
      "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
      "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
      "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
      "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
      "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
      "#Person2#: This applies to internal and external communications.\n",
      "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
      "#Person2#: Is that all?\n",
      "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
      "\n",
      "==================================================\n",
      "GROUND TRUTH SUMMARY:\n",
      "Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.\n",
      "\n",
      "==================================================\n",
      "MODEL OUTPUT (BEFORE FINE-TUNING):\n",
      "#Person1#: Ms. Dawson, I need you to take a dictation for me. #Person2#: Yes, sir... #Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. #Person2#: Yes, sir. Go ahead. #Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited. #Person2#: Sir, does this apply to intra-\n"
     ]
    }
   ],
   "source": [
    "def generate_summary(model, tokenizer, dialogue, max_new_tokens=128):\n",
    "    \"\"\"Generate a summary for a given dialogue.\"\"\"\n",
    "    prompt = f\"\"\"<|system|>\n",
    "You are a helpful assistant that summarizes conversations.</s>\n",
    "<|user|>\n",
    "Summarize the following conversation:\n",
    "\n",
    "{dialogue}</s>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "    # Set model to eval mode and disable gradient checkpointing for inference\n",
    "    model.eval()\n",
    "    if hasattr(model, 'gradient_checkpointing_disable'):\n",
    "        model.gradient_checkpointing_disable()\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            repetition_penalty=1.2,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract just the assistant's response\n",
    "    if \"<|assistant|>\" in response:\n",
    "        response = response.split(\"<|assistant|>\")[-1].strip()\n",
    "    return response\n",
    "\n",
    "# Test on a sample from test set before fine-tuning\n",
    "test_example = dataset[\"test\"][0]\n",
    "print(\"=\" * 50)\n",
    "print(\"DIALOGUE:\")\n",
    "print(test_example[\"dialogue\"])\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"GROUND TRUTH SUMMARY:\")\n",
    "print(test_example[\"summary\"])\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"MODEL OUTPUT (BEFORE FINE-TUNING):\")\n",
    "before_summary = generate_summary(model, tokenizer, test_example[\"dialogue\"])\n",
    "print(before_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e4deaf048c45b88b5c8a6c8ffab9c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39/39 02:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.597800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.560400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.564900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning complete\n",
      "Model saved to ./lora_dialogsum\n"
     ]
    }
   ],
   "source": [
    "# Ensure model is in training mode\n",
    "model.train()\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"./lora_dialogsum\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    optim=\"adamw_torch\",\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"none\",\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=1.0,\n",
    "    max_length=1024,\n",
    "    dataset_text_field=\"text\",\n",
    "    gradient_checkpointing=False,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_subset,\n",
    "    args=sft_config,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Starting fine-tuning...\")\n",
    "trainer.train()\n",
    "print(\"Fine-tuning complete\")\n",
    "\n",
    "# Save the fine-tuned LoRA adapter\n",
    "model.save_pretrained(\"./lora_dialogsum\")\n",
    "tokenizer.save_pretrained(\"./lora_dialogsum\")\n",
    "print(\"Model saved to ./lora_dialogsum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0614edac",
   "metadata": {},
   "source": [
    " model.save_pretrained(\"./lora_dialogsum\"); tokenizer.save_pretrained(\"./lora_dialogsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "test-after",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DIALOGUE:\n",
      "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
      "#Person2#: Yes, sir...\n",
      "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
      "#Person2#: Yes, sir. Go ahead.\n",
      "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
      "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
      "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
      "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
      "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
      "#Person2#: This applies to internal and external communications.\n",
      "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
      "#Person2#: Is that all?\n",
      "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
      "\n",
      "==================================================\n",
      "GROUND TRUTH SUMMARY:\n",
      "Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.\n",
      "\n",
      "==================================================\n",
      "MODEL OUTPUT (AFTER FINE-TUNING):\n",
      "#Person1#: Ms. Dawson, I need you to take a dictation for me. #Person2#: Yes, sir... #Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. #Person2#: Yes, sir. Go ahead. #Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited. #Person2#: Sir, does this apply to intra-\n"
     ]
    }
   ],
   "source": [
    "# Test after fine-tuning\n",
    "print(\"=\" * 50)\n",
    "print(\"DIALOGUE:\")\n",
    "print(test_example[\"dialogue\"])\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"GROUND TRUTH SUMMARY:\")\n",
    "print(test_example[\"summary\"])\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"MODEL OUTPUT (AFTER FINE-TUNING):\")\n",
    "after_summary = generate_summary(model, tokenizer, test_example[\"dialogue\"])\n",
    "print(after_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "more-tests",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TEST EXAMPLE 5\n",
      "============================================================\n",
      "\n",
      "DIALOGUE:\n",
      "#Person1#: You're finally here! What took so long?\n",
      "#Person2#: I got stuck in traffic again. There was a terrible traffic jam near the Carrefour intersection.\n",
      "#Person1#: It's always rather congested down there during rush hour. Maybe you should try to find a different route to get home.\n",
      "#Person2#: I don't think it can be avoided, to be honest.\n",
      "#Person1#: perhaps it would be better if you started taking public transport system to work.\n",
      "#Person2#: I think it's something that I'll have to consider. The public transport system is pretty good.\n",
      "#Person1#: It would be better for the environment, too.\n",
      "#Person2#: I know. I feel bad about how much my car is adding to the pollution problem in this city.\n",
      "#Person1#: Taking the subway would be a lot less stressful than driving as well.\n",
      "#Person2#: The only problem is that I'm going to really miss having the freedom that you have with a car.\n",
      "#Person1#: Well, when it's nicer outside, you can start biking to work. That will give you just as much freedom as your car usually provides.\n",
      "#Person2#: That's true. I could certainly use the exercise!\n",
      "#Person1#: So, are you going to quit driving to work then?\n",
      "#Person2#: Yes, it's not good for me or for the environment.\n",
      "\n",
      "GROUND TRUTH: #Person2# complains to #Person1# about the traffic jam, #Person1# suggests quitting driving and taking public transportation instead.\n",
      "\n",
      "MODEL OUTPUT: #Person1#: You're finally here! What took so long?\n",
      "#Person2#: I got stuck in traffic again. There was a terrible traffic jam near the Carrefour intersection.\n",
      "#Person1#: It's always rather congested down there during rush hour. Maybe you should try to find a different route to get home.\n",
      "#Person2#: I don't think it can be avoided, to be honest.\n",
      "#Person1#: Perhaps it would be better if you started taking public transport system to work.\n",
      "#Person2#: I think it's\n",
      "\n",
      "============================================================\n",
      "TEST EXAMPLE 10\n",
      "============================================================\n",
      "\n",
      "DIALOGUE:\n",
      "#Person1#: Happy Birthday, this is for you, Brian.\n",
      "#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n",
      "#Person1#: Brian, may I have a pleasure to have a dance with you?\n",
      "#Person2#: Ok.\n",
      "#Person1#: This is really wonderful party.\n",
      "#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n",
      "#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n",
      "#Person2#: You look great, you are absolutely glowing.\n",
      "#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n",
      "\n",
      "GROUND TRUTH: #Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n",
      "\n",
      "MODEL OUTPUT: Brian is happy to have a dance with #Person1#, and they both look good.\n",
      "\n",
      "============================================================\n",
      "TEST EXAMPLE 15\n",
      "============================================================\n",
      "\n",
      "DIALOGUE:\n",
      "#Person1#: I've had it! I am done working for a company that is taking me nowhere!\n",
      "#Person2#: So what are you gonna do? Just quit?\n",
      "#Person1#: That's exactly what I am going to do! I have decided to create my own company! I am going to write up a business plan, get some investors and start working for myself!\n",
      "#Person2#: Have you ever written up a business plan before?\n",
      "#Person1#: Well, no, it can't be that hard! I mean, all you have to do is explain your business, how you are going to do things and that's it, right?\n",
      "#Person2#: You couldn't be more wrong! A well written business plan will include an executive summary which highlights the idea of the business in two pages or less. Then you need to describe your company with information such as what type of legal structure it has, history, etc.\n",
      "#Person1#: Well, that seems easy enough.\n",
      "#Person2#: Wait, there is more! Then you need to introduce and describe your goods or services. What they are and how they are different from competitors? Then comes the hard part, a market analysis. You need to investigate and analyze hundreds of variables! You need to take into consideration socioeconomic factors from GDP per capita to how many children on average the population has! All this information is useful so that you can move on to your strategy and implementation stage, where you will describe in detail how you will actually execute your idea.\n",
      "#Person1#: Geez. Is that all?\n",
      "#Person2#: Almost, the most important piece of information for your investors will be the financial analysis. Here you will calculate and estimate sales, cash flow and profits. After all, people will want to know when they will begin to see a return on their investment!\n",
      "#Person1#: Umm. I think I ' ll just stick to my old job and save myself all the hassle of trying to start up a business!\n",
      "\n",
      "GROUND TRUTH: #Person1# wants to create a company and is going to write a business plan. #Person2# gives #Person1# suggestions on how to summarise business ideas, describe the service, differ from competitors and attract investment in a good business plan. #Person1# decides to stick to the old job.\n",
      "\n",
      "MODEL OUTPUT: #Person1#: I've had it! I am done working for a company that is taking me nowhere!\n",
      "#Person2#: So what are you gonna do? Just quit?\n",
      "#Person1#: That's exactly what I am going to do! I have decided to create my own company! I am going to write up a business plan, get some investors and start working for myself!\n",
      "#Person2#: Have you ever written up a business plan before?\n",
      "#Person1#: Well, no, it can't be that hard! I mean, all you have\n"
     ]
    }
   ],
   "source": [
    "# Test on a few more examples from the test set\n",
    "for i in [5, 10, 15]:\n",
    "    example = dataset[\"test\"][i]\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"TEST EXAMPLE {i}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nDIALOGUE:\\n{example['dialogue']}\")\n",
    "    print(f\"\\nGROUND TRUTH: {example['summary']}\")\n",
    "    print(f\"\\nMODEL OUTPUT: {generate_summary(model, tokenizer, example['dialogue'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "custom-test",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUSTOM DIALOGUE:\n",
      "#Person1#: Hey, are you coming to the party tonight?\n",
      "#Person2#: What party?\n",
      "#Person1#: Sarah's birthday party! It starts at 8pm.\n",
      "#Person2#: Oh right! I totally forgot. Where is it?\n",
      "#Person1#: At her place. I can pick you up at 7:30 if you want.\n",
      "#Person2#: That would be great, thanks!\n",
      "#Person1#: No problem. See you then!\n",
      "\n",
      "MODEL SUMMARY:\n",
      "#Person2#: Sure, see you then!\n"
     ]
    }
   ],
   "source": [
    "# Test with a custom dialogue (using training data format)\n",
    "custom_dialogue = \"\"\"#Person1#: Hey, are you coming to the party tonight?\n",
    "#Person2#: What party?\n",
    "#Person1#: Sarah's birthday party! It starts at 8pm.\n",
    "#Person2#: Oh right! I totally forgot. Where is it?\n",
    "#Person1#: At her place. I can pick you up at 7:30 if you want.\n",
    "#Person2#: That would be great, thanks!\n",
    "#Person1#: No problem. See you then!\"\"\"\n",
    "\n",
    "print(\"CUSTOM DIALOGUE:\")\n",
    "print(custom_dialogue)\n",
    "print(\"\\nMODEL SUMMARY:\")\n",
    "print(generate_summary(model, tokenizer, custom_dialogue))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

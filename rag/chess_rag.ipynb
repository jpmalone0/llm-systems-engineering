{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "403f327e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: Chess_opening\n",
      "Downloading: List_of_chess_openings\n",
      "Downloading: King%27s_Pawn_Game\n",
      "Downloading: Queen%27s_Pawn_Game\n",
      "Downloading: Open_Game\n",
      "Downloading: Semi-Open_Game\n",
      "Downloading: Closed_Game\n",
      "Downloading: Flank_opening\n",
      "Downloading: Sicilian_Defence\n",
      "Downloading: French_Defence\n",
      "Downloading: Caro%E2%80%93Kann_Defence\n",
      "Downloading: Pirc_Defence\n",
      "Downloading: Modern_Defense\n",
      "Downloading: Alekhine%27s_Defence\n",
      "Downloading: Scandinavian_Defense\n",
      "Downloading: Nimzo-Indian_Defense\n",
      "Downloading: Queen%27s_Indian_Defence\n",
      "Downloading: Bogo-Indian_Defense\n",
      "Downloading: King%27s_Indian_Defence\n",
      "Downloading: Gr%C3%BCnfeld_Defence\n",
      "Downloading: Benoni_Defense\n",
      "Downloading: Benko_Gambit\n",
      "Downloading: Dutch_Defense\n",
      "Downloading: Slav_Defense\n",
      "Downloading: Semi-Slav_Defense\n",
      "Downloading: Queen%27s_Gambit\n",
      "Downloading: Catalan_Opening\n",
      "Downloading: Ruy_Lopez\n",
      "Downloading: Italian_Game\n",
      "Downloading: Scotch_Game\n",
      "Downloading: Four_Knights_Game\n",
      "Downloading: Vienna_Game\n",
      "Downloading: English_Opening\n",
      "Downloading: R%C3%A9ti_Opening\n",
      "Downloading: Bird%27s_Opening\n",
      "Downloading: London_System\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import urllib.parse\n",
    "\n",
    "# Used ChatGPT to generate this list of pages and download logic\n",
    "wiki_pages = [\n",
    "    \"Chess_opening\",\n",
    "    \"List_of_chess_openings\",\n",
    "    \"King%27s_Pawn_Game\",\n",
    "    \"Queen%27s_Pawn_Game\",\n",
    "    \"Open_Game\",\n",
    "    \"Semi-Open_Game\",\n",
    "    \"Closed_Game\",\n",
    "    \"Flank_opening\",\n",
    "\n",
    "    \"Sicilian_Defence\",\n",
    "    \"French_Defence\",\n",
    "    \"Caro%E2%80%93Kann_Defence\",\n",
    "    \"Pirc_Defence\",\n",
    "    \"Modern_Defense\",\n",
    "    \"Alekhine%27s_Defence\",\n",
    "    \"Scandinavian_Defense\",\n",
    "\n",
    "    \"Nimzo-Indian_Defense\",\n",
    "    \"Queen%27s_Indian_Defence\",\n",
    "    \"Bogo-Indian_Defense\",\n",
    "    \"King%27s_Indian_Defence\",\n",
    "    \"Gr%C3%BCnfeld_Defence\",\n",
    "    \"Benoni_Defense\",\n",
    "    \"Benko_Gambit\",\n",
    "    \"Dutch_Defense\",\n",
    "    \"Slav_Defense\",\n",
    "    \"Semi-Slav_Defense\",\n",
    "    \"Queen%27s_Gambit\",\n",
    "    \"Catalan_Opening\",\n",
    "\n",
    "    \"Ruy_Lopez\",\n",
    "    \"Italian_Game\",\n",
    "    \"Scotch_Game\",\n",
    "    \"Four_Knights_Game\",\n",
    "    \"Vienna_Game\",\n",
    "    \"English_Opening\",\n",
    "    \"R%C3%A9ti_Opening\",\n",
    "    \"Bird%27s_Opening\",\n",
    "    \"London_System\"\n",
    "]\n",
    "\n",
    "kb_dir = \"/Users/jpmalone/Documents/Applied_ML/applied_ml_hw4/kb\"\n",
    "os.makedirs(kb_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"applied_ml_hw4/1.0 (contact: jpmalone@iu.edu)\"\n",
    "}\n",
    "\n",
    "def download_wiki_page(title):\n",
    "    # Decode %27, %C3%BC, %E2%80%93, etc.\n",
    "    decoded_title = urllib.parse.unquote(title)\n",
    "\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "        \"exsectionformat\": \"plain\",\n",
    "        \"redirects\": 1,\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": decoded_title,\n",
    "    }\n",
    "\n",
    "    r = requests.get(\n",
    "        \"https://en.wikipedia.org/w/api.php\",\n",
    "        params=params,\n",
    "        headers=HEADERS,\n",
    "    )\n",
    "    r.raise_for_status()\n",
    "\n",
    "    pages = r.json()[\"query\"][\"pages\"]\n",
    "    page = next(iter(pages.values()))\n",
    "\n",
    "    if \"missing\" in page:\n",
    "        raise ValueError(f\"Page not found: {decoded_title}\")\n",
    "\n",
    "    if \"extract\" not in page or not page[\"extract\"].strip():\n",
    "        raise ValueError(f\"No extract available for page: {decoded_title}\")\n",
    "\n",
    "    return page[\"extract\"]\n",
    "\n",
    "for title in wiki_pages:\n",
    "    print(f\"Downloading: {title}\")\n",
    "    try:\n",
    "        text = download_wiki_page(title)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipped: {e}\")\n",
    "        continue\n",
    "\n",
    "    filename = (\n",
    "        title.lower()\n",
    "        .replace(\"%27\", \"\")\n",
    "        .replace(\"%e2%80%93\", \"-\")\n",
    "        .replace(\"%c3%bc\", \"u\")\n",
    "        + \".txt\"\n",
    "    )\n",
    "\n",
    "    path = os.path.join(kb_dir, filename)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c5f3cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jpmalone/Documents/Repos/llm-systems-engineering/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Batches: 100%|██████████| 28/28 [00:18<00:00,  1.54it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import faiss\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "LLM_NAME = \"google/gemma-3-1b-it\"\n",
    "EMB_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLM_NAME)\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_NAME,\n",
    "    dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    low_cpu_mem_usage=True,\n",
    ").to(device)\n",
    "llm.eval()\n",
    "\n",
    "embedder = SentenceTransformer(EMB_NAME, device=\"cpu\")\n",
    "\n",
    "def load_texts_from_dir(dir_path):\n",
    "    texts = []\n",
    "    sources = []\n",
    "    for fn in sorted(os.listdir(dir_path)):\n",
    "        if fn.lower().endswith(\".txt\"):\n",
    "            with open(os.path.join(dir_path, fn), \"r\", encoding=\"utf-8\") as f:\n",
    "                texts.append(f.read())\n",
    "            sources.append(fn)\n",
    "    return texts, sources\n",
    "\n",
    "def chunk_text(text, chunk_size=800, overlap=150):\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(len(text), start + chunk_size)\n",
    "        chunks.append(text[start:end])\n",
    "        if end == len(text):\n",
    "            break\n",
    "        start = end - overlap\n",
    "    return chunks\n",
    "\n",
    "raw_texts, raw_sources = load_texts_from_dir(kb_dir)\n",
    "\n",
    "chunks = []\n",
    "chunk_meta = []\n",
    "for src, doc in zip(raw_sources, raw_texts):\n",
    "    for i, c in enumerate(chunk_text(doc)):\n",
    "        chunks.append(c)\n",
    "        chunk_meta.append((src, i))\n",
    "\n",
    "chunk_emb = embedder.encode(\n",
    "    chunks,\n",
    "    batch_size=32,\n",
    "    normalize_embeddings=True,\n",
    "    convert_to_numpy=True,\n",
    "    show_progress_bar=True,\n",
    ")\n",
    "\n",
    "dim = chunk_emb.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "index.add(chunk_emb)\n",
    "\n",
    "def retrieve(query, k=5):\n",
    "    q_emb = embedder.encode(\n",
    "        [query],\n",
    "        batch_size=1,\n",
    "        normalize_embeddings=True,\n",
    "        convert_to_numpy=True,\n",
    "    )\n",
    "    scores, idx = index.search(q_emb, k)\n",
    "    return [\n",
    "        {\n",
    "            \"score\": float(s),\n",
    "            \"source\": chunk_meta[j][0],\n",
    "            \"chunk_id\": chunk_meta[j][1],\n",
    "            \"text\": chunks[j],\n",
    "        }\n",
    "        for s, j in zip(scores[0], idx[0])\n",
    "    ]\n",
    "\n",
    "def generate_rag_answer(query, k=5, max_new_tokens=250):\n",
    "    retrieved = retrieve(query, k)\n",
    "\n",
    "    context = \"\\n\\n\".join(\n",
    "        f\"[Source: {r['source']} | chunk {r['chunk_id']}]\\n{r['text']}\"\n",
    "        for r in retrieved\n",
    "    )\n",
    "\n",
    "    prompt = (\n",
    "        \"Answer using ONLY the CONTEXT.\\n\"\n",
    "        \"If the answer is not in the context, output exactly:\\n\"\n",
    "        \"I don't know based on the provided sources.\\n\\n\"\n",
    "        f\"CONTEXT:\\n{context}\\n\\n\"\n",
    "        f\"QUESTION:\\n{query}\\n\\n\"\n",
    "        \"ANSWER:\\n\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out = llm.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    return text.split(\"ANSWER:\\n\", 1)[-1].strip(), retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b3cbf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:\n",
      " The main strategic ideas of the Sicilian Defense are:\n",
      "1.  White pushes a kingside pawn, asserting control over the d4 square.\n",
      "2.  White develops the king's knight with 2.Nf3.\n",
      "3.  Black replies with 2...Nc6, 2...d6, or 2...e6.\n",
      "4.  White often holds the initiative on the side of the board.\n",
      "5.  Black needs to avoid a quick attack.\n",
      "\n",
      "SOURCES:\n",
      "- sicilian_defence.txt (chunk 9, score=0.633)\n",
      "- sicilian_defence.txt (chunk 0, score=0.626)\n",
      "- sicilian_defence.txt (chunk 2, score=0.615)\n",
      "- english_opening.txt (chunk 8, score=0.613)\n",
      "- sicilian_defence.txt (chunk 15, score=0.602)\n"
     ]
    }
   ],
   "source": [
    "answer, sources = generate_rag_answer(\n",
    "    \"What are the main strategic ideas of the Sicilian Defense?\",\n",
    "    k=5\n",
    ")\n",
    "\n",
    "answer = answer.split(\"\\n\\n\", 1)[0].strip()  \n",
    "\n",
    "print(\"ANSWER:\\n\", answer)\n",
    "print(\"\\nSOURCES:\")\n",
    "for s in sources:\n",
    "    print(f\"- {s['source']} (chunk {s['chunk_id']}, score={s['score']:.3f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
